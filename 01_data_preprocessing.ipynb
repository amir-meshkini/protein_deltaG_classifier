{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5f61976",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/amir-meshkini/protein_deltaG_classifier/blob/main/01_data_preprocessing.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70c0d16",
   "metadata": {},
   "source": [
    "# ΔG Prediction: Preprocessing Notebook \n",
    "\n",
    "In this notebook the first 200000 record of the PSPD dataset are tokenized and vectorized, using the Facebook's `EMS2 8M` PLP (Protein Language Processing) model. \n",
    "\n",
    "The vectorized data meant to be used later as the training data for our ΔG classifier prediction model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bae0af2",
   "metadata": {},
   "source": [
    "### Importing the packages \n",
    "\n",
    "A full list of required packages and their corresponding versions is provided in the `requirements.txt` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e14859a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import math\n",
    "import tensorflow as tf \n",
    "from transformers import AutoTokenizer, TFEsmModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0749e72",
   "metadata": {},
   "source": [
    "### Importing the dataset\n",
    "\n",
    "Here, the `smallpspd.csv` is simply the first 200000 rows of the [original PSPD dataset](https://huggingface.co/datasets/benchang323/protein-stability-prediction) ([licensed by MIT](https://choosealicense.com/licenses/mit/)) which has over 4 million records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a66ef36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 200000 entries, 0 to 199999\n",
      "Data columns (total 4 columns):\n",
      " #   Column      Non-Null Count   Dtype  \n",
      "---  ------      --------------   -----  \n",
      " 0   Unnamed: 0  200000 non-null  int64  \n",
      " 1   aa_seq      200000 non-null  object \n",
      " 2   deltaG      200000 non-null  float64\n",
      " 3   deltaG_bin  200000 non-null  object \n",
      "dtypes: float64(1), int64(1), object(2)\n",
      "memory usage: 6.1+ MB\n"
     ]
    }
   ],
   "source": [
    "pspd = pd.read_csv('smallpspd.csv') \n",
    "pspd.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc6e35d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'MKIFVKTLTGKTITLEVEPSDTIENVKAKIQDEEGIPPDQQRLIFAGKKLEDGRTLTDYSIQKESTLHLVLR'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences = list(pspd['aa_seq'])\n",
    "\n",
    "# A sample of the amino acid sequences\n",
    "sequences[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd11e9b",
   "metadata": {},
   "source": [
    "### Encoding the labels\n",
    "\n",
    "The ΔG values of this dataset lies between -10 to +15 Kcal/mol. therefore 26 different classes are considered that correspond to 26 different intervals of integers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f1f86ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>deltaG (Kcal/mol)</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8 to 9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5 to 6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0 to 1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-4 to -3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5 to 6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3 to 4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3 to 4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4 to 5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1 to 2</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>12 to 13</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  deltaG (Kcal/mol)  labels\n",
       "0            8 to 9       0\n",
       "1            5 to 6       1\n",
       "2            0 to 1       2\n",
       "3          -4 to -3       3\n",
       "4            5 to 6       1\n",
       "5            3 to 4       4\n",
       "6            3 to 4       4\n",
       "7            4 to 5       5\n",
       "8            1 to 2       6\n",
       "9          12 to 13       7"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Giving each interval a label (integers from 0 to 25)\n",
    "delg_range = pspd['deltaG_bin'] \n",
    "label, interval = pd.factorize(delg_range) \n",
    "delg_range_labels = pd.DataFrame({'deltaG (Kcal/mol)': delg_range, 'labels': label}) \n",
    "\n",
    "delg_range_labels.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d4732c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '8 to 9',\n",
       " 1: '5 to 6',\n",
       " 2: '0 to 1',\n",
       " 3: '-4 to -3',\n",
       " 4: '3 to 4',\n",
       " 5: '4 to 5',\n",
       " 6: '1 to 2',\n",
       " 7: '12 to 13',\n",
       " 8: '-5 to -4',\n",
       " 9: '-2 to -1',\n",
       " 10: '10 to 11',\n",
       " 11: '11 to 12',\n",
       " 12: '-9 to -8',\n",
       " 13: '-7 to -6',\n",
       " 14: '-8 to -7',\n",
       " 15: '-10 to -9',\n",
       " 16: '13 to 14',\n",
       " 17: '-3 to -2',\n",
       " 18: '2 to 3',\n",
       " 19: '-6 to -5',\n",
       " 20: '9 to 10',\n",
       " 21: '7 to 8',\n",
       " 22: '15 to 16',\n",
       " 23: '14 to 15',\n",
       " 24: '6 to 7',\n",
       " 25: '-1 to 0'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "code_to_value = dict(enumerate(interval))\n",
    "code_to_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a84a73c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_data = delg_range_labels['labels'].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35f79fe",
   "metadata": {},
   "source": [
    "### Using the pre-trained models \n",
    "\n",
    "After initializing the tokenizer and the EMS model, A function is defined to feed the data to the model in chunks, in order to avoid getting the `ResourceExhaustedError` \n",
    "\n",
    "> **Note:** If you wish to rerun this code on your local machine, you might be having to adjust the patch size, considering your own RAM (or VRAM if you'r using CUDA) to avoid getting the mentioned error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "472a0645",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\amesh\\anaconda3\\envs\\cvproj\\lib\\site-packages\\huggingface_hub\\file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFEsmModel: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.bias', 'esm.embeddings.position_ids', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'esm.embeddings.position_embeddings.weight']\n",
      "- This IS expected if you are initializing TFEsmModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFEsmModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFEsmModel were not initialized from the PyTorch model and are newly initialized: ['esm.pooler.dense.weight', 'esm.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# using EMS2 model with 8 million parameters\n",
    "model_name = \"facebook/esm2_t6_8M_UR50D\"\n",
    "\n",
    "# Initializing the tokenizer and the EMS model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name) \n",
    "model = TFEsmModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2469b490",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 200000 sequences in 200 batches...\n",
      "Batch 1/200 done.\n",
      "Batch 2/200 done.\n",
      "Batch 3/200 done.\n",
      "Batch 4/200 done.\n",
      "Batch 5/200 done.\n",
      "Batch 6/200 done.\n",
      "Batch 7/200 done.\n",
      "Batch 8/200 done.\n",
      "Batch 9/200 done.\n",
      "Batch 10/200 done.\n",
      "Batch 11/200 done.\n",
      "Batch 12/200 done.\n",
      "Batch 13/200 done.\n",
      "Batch 14/200 done.\n",
      "Batch 15/200 done.\n",
      "Batch 16/200 done.\n",
      "Batch 17/200 done.\n",
      "Batch 18/200 done.\n",
      "Batch 19/200 done.\n",
      "Batch 20/200 done.\n",
      "Batch 21/200 done.\n",
      "Batch 22/200 done.\n",
      "Batch 23/200 done.\n",
      "Batch 24/200 done.\n",
      "Batch 25/200 done.\n",
      "Batch 26/200 done.\n",
      "Batch 27/200 done.\n",
      "Batch 28/200 done.\n",
      "Batch 29/200 done.\n",
      "Batch 30/200 done.\n",
      "Batch 31/200 done.\n",
      "Batch 32/200 done.\n",
      "Batch 33/200 done.\n",
      "Batch 34/200 done.\n",
      "Batch 35/200 done.\n",
      "Batch 36/200 done.\n",
      "Batch 37/200 done.\n",
      "Batch 38/200 done.\n",
      "Batch 39/200 done.\n",
      "Batch 40/200 done.\n",
      "Batch 41/200 done.\n",
      "Batch 42/200 done.\n",
      "Batch 43/200 done.\n",
      "Batch 44/200 done.\n",
      "Batch 45/200 done.\n",
      "Batch 46/200 done.\n",
      "Batch 47/200 done.\n",
      "Batch 48/200 done.\n",
      "Batch 49/200 done.\n",
      "Batch 50/200 done.\n",
      "Batch 51/200 done.\n",
      "Batch 52/200 done.\n",
      "Batch 53/200 done.\n",
      "Batch 54/200 done.\n",
      "Batch 55/200 done.\n",
      "Batch 56/200 done.\n",
      "Batch 57/200 done.\n",
      "Batch 58/200 done.\n",
      "Batch 59/200 done.\n",
      "Batch 60/200 done.\n",
      "Batch 61/200 done.\n",
      "Batch 62/200 done.\n",
      "Batch 63/200 done.\n",
      "Batch 64/200 done.\n",
      "Batch 65/200 done.\n",
      "Batch 66/200 done.\n",
      "Batch 67/200 done.\n",
      "Batch 68/200 done.\n",
      "Batch 69/200 done.\n",
      "Batch 70/200 done.\n",
      "Batch 71/200 done.\n",
      "Batch 72/200 done.\n",
      "Batch 73/200 done.\n",
      "Batch 74/200 done.\n",
      "Batch 75/200 done.\n",
      "Batch 76/200 done.\n",
      "Batch 77/200 done.\n",
      "Batch 78/200 done.\n",
      "Batch 79/200 done.\n",
      "Batch 80/200 done.\n",
      "Batch 81/200 done.\n",
      "Batch 82/200 done.\n",
      "Batch 83/200 done.\n",
      "Batch 84/200 done.\n",
      "Batch 85/200 done.\n",
      "Batch 86/200 done.\n",
      "Batch 87/200 done.\n",
      "Batch 88/200 done.\n",
      "Batch 89/200 done.\n",
      "Batch 90/200 done.\n",
      "Batch 91/200 done.\n",
      "Batch 92/200 done.\n",
      "Batch 93/200 done.\n",
      "Batch 94/200 done.\n",
      "Batch 95/200 done.\n",
      "Batch 96/200 done.\n",
      "Batch 97/200 done.\n",
      "Batch 98/200 done.\n",
      "Batch 99/200 done.\n",
      "Batch 100/200 done.\n",
      "Batch 101/200 done.\n",
      "Batch 102/200 done.\n",
      "Batch 103/200 done.\n",
      "Batch 104/200 done.\n",
      "Batch 105/200 done.\n",
      "Batch 106/200 done.\n",
      "Batch 107/200 done.\n",
      "Batch 108/200 done.\n",
      "Batch 109/200 done.\n",
      "Batch 110/200 done.\n",
      "Batch 111/200 done.\n",
      "Batch 112/200 done.\n",
      "Batch 113/200 done.\n",
      "Batch 114/200 done.\n",
      "Batch 115/200 done.\n",
      "Batch 116/200 done.\n",
      "Batch 117/200 done.\n",
      "Batch 118/200 done.\n",
      "Batch 119/200 done.\n",
      "Batch 120/200 done.\n",
      "Batch 121/200 done.\n",
      "Batch 122/200 done.\n",
      "Batch 123/200 done.\n",
      "Batch 124/200 done.\n",
      "Batch 125/200 done.\n",
      "Batch 126/200 done.\n",
      "Batch 127/200 done.\n",
      "Batch 128/200 done.\n",
      "Batch 129/200 done.\n",
      "Batch 130/200 done.\n",
      "Batch 131/200 done.\n",
      "Batch 132/200 done.\n",
      "Batch 133/200 done.\n",
      "Batch 134/200 done.\n",
      "Batch 135/200 done.\n",
      "Batch 136/200 done.\n",
      "Batch 137/200 done.\n",
      "Batch 138/200 done.\n",
      "Batch 139/200 done.\n",
      "Batch 140/200 done.\n",
      "Batch 141/200 done.\n",
      "Batch 142/200 done.\n",
      "Batch 143/200 done.\n",
      "Batch 144/200 done.\n",
      "Batch 145/200 done.\n",
      "Batch 146/200 done.\n",
      "Batch 147/200 done.\n",
      "Batch 148/200 done.\n",
      "Batch 149/200 done.\n",
      "Batch 150/200 done.\n",
      "Batch 151/200 done.\n",
      "Batch 152/200 done.\n",
      "Batch 153/200 done.\n",
      "Batch 154/200 done.\n",
      "Batch 155/200 done.\n",
      "Batch 156/200 done.\n",
      "Batch 157/200 done.\n",
      "Batch 158/200 done.\n",
      "Batch 159/200 done.\n",
      "Batch 160/200 done.\n",
      "Batch 161/200 done.\n",
      "Batch 162/200 done.\n",
      "Batch 163/200 done.\n",
      "Batch 164/200 done.\n",
      "Batch 165/200 done.\n",
      "Batch 166/200 done.\n",
      "Batch 167/200 done.\n",
      "Batch 168/200 done.\n",
      "Batch 169/200 done.\n",
      "Batch 170/200 done.\n",
      "Batch 171/200 done.\n",
      "Batch 172/200 done.\n",
      "Batch 173/200 done.\n",
      "Batch 174/200 done.\n",
      "Batch 175/200 done.\n",
      "Batch 176/200 done.\n",
      "Batch 177/200 done.\n",
      "Batch 178/200 done.\n",
      "Batch 179/200 done.\n",
      "Batch 180/200 done.\n",
      "Batch 181/200 done.\n",
      "Batch 182/200 done.\n",
      "Batch 183/200 done.\n",
      "Batch 184/200 done.\n",
      "Batch 185/200 done.\n",
      "Batch 186/200 done.\n",
      "Batch 187/200 done.\n",
      "Batch 188/200 done.\n",
      "Batch 189/200 done.\n",
      "Batch 190/200 done.\n",
      "Batch 191/200 done.\n",
      "Batch 192/200 done.\n",
      "Batch 193/200 done.\n",
      "Batch 194/200 done.\n",
      "Batch 195/200 done.\n",
      "Batch 196/200 done.\n",
      "Batch 197/200 done.\n",
      "Batch 198/200 done.\n",
      "Batch 199/200 done.\n",
      "Batch 200/200 done.\n",
      "Final shape: (200000, 320)\n"
     ]
    }
   ],
   "source": [
    "def get_embeddings_in_batches(sequence_list, batch_size=8):\n",
    "    all_vectors = []\n",
    "    \n",
    "    total_batches = math.ceil(len(sequence_list) / batch_size)\n",
    "    print(f\"Processing {len(sequence_list)} sequences in {total_batches} batches...\")\n",
    "\n",
    "    for i in range(0, len(sequence_list), batch_size):\n",
    "        # Create the batch\n",
    "        batch_sequences = sequence_list[i : i + batch_size]\n",
    "        \n",
    "        # Tokenize ONLY this batch\n",
    "        # max_length=1024 ensures super long proteins don't crash memory\n",
    "        inputs = tokenizer(batch_sequences, return_tensors=\"tf\", padding=True, truncation=True, max_length=1024)\n",
    "        \n",
    "        # Inference\n",
    "        outputs = model(inputs, training=False)\n",
    "        \n",
    "        # Pooling (same logic as before)\n",
    "        token_embeddings = outputs.last_hidden_state\n",
    "        attention_mask = inputs['attention_mask']\n",
    "        \n",
    "        mask = tf.cast(tf.expand_dims(attention_mask, -1), tf.float32)\n",
    "        sum_embeddings = tf.reduce_sum(token_embeddings * mask, axis=1)\n",
    "        sum_mask = tf.reduce_sum(mask, axis=1)\n",
    "        \n",
    "        # Calculate mean vectors for this batch\n",
    "        batch_vectors = sum_embeddings / tf.maximum(sum_mask, 1e-9)\n",
    "        \n",
    "        # Add to the main list\n",
    "        all_vectors.append(batch_vectors.numpy())\n",
    "        \n",
    "        print(f\"Batch {i//batch_size + 1}/{total_batches} done.\")\n",
    "\n",
    "    # Combine all batches into one big matrix\n",
    "    return np.concatenate(all_vectors, axis=0)\n",
    "\n",
    "# 4. Run extraction\n",
    "# Try batch_size=4 or batch_size=2 if you still get errors\n",
    "X_data = get_embeddings_in_batches(sequences, batch_size=1000)\n",
    "\n",
    "print(f\"Final shape: {X_data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0221c0c5",
   "metadata": {},
   "source": [
    "### Saving the arrays to be used later \n",
    "\n",
    "In order to keep the data files relatively small (> 100MB's) for easier version controlling, the X_data is cut into 4 chunks to be concatenated later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bcb9e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = X_data[:50000]\n",
    "x2 = X_data[50000:100000]\n",
    "x3 = X_data[100000:150000]\n",
    "x4 = X_data[150000:] \n",
    "np.save('X1.npy', x1) \n",
    "np.save('X2.npy', x2) \n",
    "np.save('X3.npy', x3)\n",
    "np.save('X4.npy', x4)\n",
    "np.save(file='y.npy', arr=y_data) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cvproj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
